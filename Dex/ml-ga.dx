'# ML via gradient ascent using Dex

-- load some generic utility functions (need a TSV parser)
import djwutils

'## Read and process the data

dat = unsafe_io \. read_file "../pima.data"
AsList(_, tab) = parse_tsv ' ' dat
atab = map (\l. cons "1.0" l) tab
att = map (\r. list2tab r :: (Fin 9)=>String) atab
xStr = map (\r. slice r 0 (Fin 8)) att
xmb = map (\r. map parseString r) xStr :: _=>(Fin 8)=>(Maybe Float)
x = map (\r. map from_just r) xmb :: _=>(Fin 8)=>Float
yStrM = map (\r. slice r 8 (Fin 1)) att
yStr = (transpose yStrM)[0@_]
y = map (\s. select (s == "Yes") 1.0 0.0) yStr

x
y

'## Gradient ascent

def ll(b: (Fin 8)=>Float) -> Float =
  neg $ sum (log (map (\ x. (exp x) + 1) ((map (\ yi. 1 - 2*yi) y)*(x **. b))))

gll = \x. grad ll x -- use auto-diff for the gradient

def one_step(learning_rate: Float) -> (Fin 8=>Float) -> (Fin 8)=>Float =
  \b0. b0 + learning_rate .* gll b0

def ascend(step: (Fin 8=>Float) -> (Fin 8)=>Float, init: (Fin 8)=>Float, max_its: Float) ->
    (Fin 8)=>Float =
  (b_opt, its_left) = yield_state (init, max_its) \state.
    while \.
      (b0, its) = get state
      b1 = step b0
      diff = b1-b0
      sz = sqrt $ sum $ diff*diff
      if ((its > 0) && (sz > 1.0e-8))
        then
          state := (b1, its - 1)
          True
        else False
  b_opt

init = [-9.8, 0.1, 0, 0, 0, 0, 1.8, 0]
ll init

opt = ascend (one_step 1.0e-6) init 10000
opt
ll opt

-- eof
